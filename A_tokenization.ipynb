{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "from functions import make_ngram, uptoNgram\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bow import BagOfWords, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 dataset_split_imdb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 tokenize_imdb.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load tokenized dataset and labels\n",
    "##spacy tokenization\n",
    "\n",
    "train_data_tokens = pk.load(open(\"train_data_tokens_spacy.pk\", \"rb\"))\n",
    "val_data_tokens = pk.load(open(\"val_data_tokens_spacy.pk\", \"rb\"))\n",
    "test_data_tokens = pk.load(open(\"test_data_tokens_spacy.pk\", \"rb\"))\n",
    "train_label = pk.load(open(\"train_label.pk\", \"rb\"))\n",
    "test_label = pk.load(open(\"test_label.pk\", \"rb\"))\n",
    "val_label = pk.load(open(\"val_label.pk\", \"rb\"))\n",
    "\n",
    "ngram_dct = {}\n",
    "ngram_dct['train_1'] = train_data_tokens\n",
    "ngram_dct['test_1'] = test_data_tokens\n",
    "ngram_dct['val_1'] = val_data_tokens\n",
    "\n",
    "#create ngrams for train, test, val dataset\n",
    "for dataset in ['train', 'test', 'val']:\n",
    "    for n in range(2,5):\n",
    "        ngram_dct['{}_{}'.format(dataset, n)] = make_ngram(n, ngram_dct[dataset+'_1'])\n",
    "\n",
    "print(ngram_dct.keys())\n",
    "pk.dump(ngram_dct, open(\"ngram_dct.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train_1', 'test_1', 'val_1', 'train_2', 'train_3', 'train_4', 'test_2', 'test_3', 'test_4', 'val_2', 'val_3', 'val_4'])\n"
     ]
    }
   ],
   "source": [
    "train_data_tokens = pk.load(open(\"train_data_tokens_nltk.pk\", \"rb\"))\n",
    "val_data_tokens = pk.load(open(\"val_data_tokens_nltk.pk\", \"rb\"))\n",
    "test_data_tokens = pk.load(open(\"test_data_tokens_nltk.pk\", \"rb\"))\n",
    "ngram_dct_nltk = {}\n",
    "ngram_dct_nltk['train_1'] = train_data_tokens\n",
    "ngram_dct_nltk['test_1'] = test_data_tokens\n",
    "ngram_dct_nltk['val_1'] = val_data_tokens\n",
    "\n",
    "#create ngrams for train, test, val dataset\n",
    "for dataset in ['train', 'test', 'val']:\n",
    "    for n in range(2,5):\n",
    "        ngram_dct_nltk['{}_{}'.format(dataset, n)] = make_ngram(n, ngram_dct_nltk[dataset+'_1'])\n",
    "\n",
    "print(ngram_dct_nltk.keys())\n",
    "pk.dump(ngram_dct_nltk, open(\"ngram_dct_nltk.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy tokenization:  ['pretty', 'poor', 'firestarter', 'clone', 'that', 'seems', 'more', 'like', 'a', 'bad', 'tv', 'movie', 'than', 'a', 'bad', 'feature', 'film', 'how', 'disappointing', 'for', 'this', 'to', 'come', 'from', 'hooper', 'and', 'dourif!<br', '/><br', '/>government', 'contractors', 'do', 'a', 'human', 'experiment', 'with', 'a', 'hydrogen', 'bomb', 'the', 'boy', 'born', 'to', 'the', 'couple', 'from', 'the', 'experiment', 'constantly', 'runs', 'a', 'fever', 'of', '100', 'degrees', 'and', 'when', 'he', \"'s\", 'an', 'adult', 'people', 'in', 'his', 'life', 'start', 'spontaneously', 'combusting', 'he', 'tries', 'to', 'find', 'out', 'why.<br', '/><br', '/>the', 'people', 'completely', 'on', 'fire', 'are', 'well', 'done', 'but', 'when', 'they', 'get', 'to', 'the', 'point', 'that', 'they', 'are', 'well', 'done', 'in', 'another', 'sense', 'they', \"'re\", 'obviously', 'changed', 'to', 'dummies', 'when', 'jets', 'of', 'fire', 'shoot', 'out', 'of', 'characters', 'arms', 'it', 'looks', 'silly', 'rather', 'than', 'alarming', 'the', 'way', 'it', 'should', 'also', 'ridiculous', 'is', 'fire', 'that', 'evidently', 'travels', 'through', 'phone', 'lines', 'and', 'erupts', 'in', 'huge', 'jets', 'from', 'the', 'receiver', \"'s\", 'earpiece', 'how', 'is', 'that', 'supposed', 'to', 'happen', 'exactly?<br', '/><br', '/>something', 'else', 'that', 'struck', 'me', 'as', 'silly', 'about', 'the', 'movie', 'is', 'when', 'a', 'character', 'has', 'visions', 'of', 'his', 'late', 'parents', 'we', 'later', 'see', 'the', 'exact', 'same', 'shots', 'from', 'those', 'visions', 'in', 'home', 'movies'] \n",
      "\n",
      "nltk stemming tokenization:  ['pretti', 'poor', 'firestart', 'clone', 'seem', 'like', 'bad', 'tv', 'movi', 'bad', 'featur', 'film', 'disappoint', 'thi', 'come', 'hooper', 'dourif', 'br', 'br', 'govern', 'contractor', 'human', 'experi', 'hydrogen', 'bomb', 'boy', 'born', 'coupl', 'experi', 'constantli', 'run', 'fever', 'degre', 'adult', 'peopl', 'hi', 'life', 'start', 'spontan', 'combust', 'tri', 'find', 'br', 'br', 'peopl', 'complet', 'fire', 'well', 'done', 'get', 'point', 'well', 'done', 'anoth', 'sens', 'obvious', 'chang', 'dummi', 'jet', 'fire', 'shoot', 'charact', 'arm', 'look', 'silli', 'rather', 'alarm', 'way', 'also', 'ridicul', 'fire', 'evid', 'travel', 'phone', 'line', 'erupt', 'huge', 'jet', 'receiv', 'earpiec', 'suppos', 'happen', 'exactli', 'br', 'br', 'someth', 'els', 'struck', 'silli', 'movi', 'charact', 'ha', 'vision', 'hi', 'late', 'parent', 'later', 'see', 'exact', 'shot', 'vision', 'home', 'movi']\n"
     ]
    }
   ],
   "source": [
    "print('spacy tokenization: ', ngram_dct['train_1'][10], '\\n')\n",
    "print('nltk stemming tokenization: ', ngram_dct_nltk['train_1'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include 1 to ngrams as features\n",
    "# upto2gram = uptoNgram(2, ngram_dct)\n",
    "# upto3gram = uptoNgram(3, ngram_dct)\n",
    "# upto4gram = uptoNgram(4, ngram_dct)\n",
    "# pk.dump(upto2gram, open(\"upto2gram.pk\", \"wb\"))\n",
    "# pk.dump(upto3gram, open(\"upto3gram.pk\", \"wb\"))\n",
    "# pk.dump(upto4gram, open(\"upto4gram.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sanity check\n",
    "# n = 4\n",
    "# dataset_ind = np.random.randint(3)\n",
    "# dataset = ['train', 'val', 'test'][dataset_ind]\n",
    "# x = np.random.randint(len(ngram_dct['%s_1'%dataset]))\n",
    "\n",
    "# s = 0\n",
    "# for i in range(1,n+1):\n",
    "#     s += len(ngram_dct['%s_%s'%(dataset,i)][x])\n",
    "\n",
    "# assert(len(upto4gram[dataset][x])==s)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
